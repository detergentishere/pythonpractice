#âœ¨ğŸ¼ Iris Logistic Regression Classifier ğŸ¼âœ¨
#Weâ€™ll train Logistic Regression on ALL 4 features of the Iris dataset ğŸŒ¸
#Then reduce data to 2D with PCA just for visualization ğŸ¨
#Letâ€™s see how well our model separates flowers ğŸ’–ğŸŒ¼

#ğŸŒŸ Imports
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

#ğŸŒ¸ Load dataset (Iris has 150 flowers, 3 species, 4 features each)
iris=load_iris()
X, y=iris.data, iris.target
print("ğŸŒ¼ Features:", iris.feature_names)
print("ğŸŒ¸ Classes:", iris.target_names)

#ğŸ§© Split dataset â†’ Train (80%) + Test (20%)
X_train, X_test, y_train, y_test =train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

#ğŸŒ€ Scale features â†’ standardize (mean=0, variance=1)
scaler = StandardScaler()
X_train_scaled= scaler.fit_transform(X_train)
X_test_scaled=scaler.transform(X_test)

#ğŸ’™ Train Logistic Regression on ALL 4 features
clf=LogisticRegression(max_iter=200, random_state=42, multi_class="multinomial")
clf.fit(X_train_scaled, y_train)

#ğŸŒ¸ Evaluate the model
print("ğŸ’– Train accuracy:", clf.score(X_train_scaled, y_train))
print("ğŸ’– Test accuracy:", clf.score(X_test_scaled, y_test))

#ğŸ Try predicting one flower
sample=X_test[0].reshape(1, -1)
print("ğŸŒ¼ Sample flower features:", sample)
print("ğŸŒ¸ True label:", iris.target_names[y_test[0]])
print("ğŸŒ¸ Predicted label:", iris.target_names[clf.predict(sample)[0]])

#ğŸ¨ Visualization with PCA
#Reduce 4D â†’ 2D (just for human eyes ğŸ‘€âœ¨)
pca=PCA(n_components=2)
X_pca=pca.fit_transform(scaler.transform(X))

#ğŸŒ€ Make a mesh grid in PCA space (like painting a canvas ğŸ¨)
x_min, x_max=X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max=X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                     np.linspace(y_min, y_max, 200))

#ğŸŒˆ Transform grid back into 4D â†’ scale â†’ predict class
grid_pca=np.c_[xx.ravel(), yy.ravel()]
grid_original=pca.inverse_transform(grid_pca)
Z=clf.predict(scaler.transform(grid_original))
Z=Z.reshape(xx.shape)

#ğŸ¨ Plot decision regions + flowers
plt.figure(figsize=(8,6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Set3)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor="k", cmap=plt.cm.Set3, s=60)

plt.xlabel("PCA Feature 1 ğŸŒ¼")
plt.ylabel("PCA Feature 2 ğŸŒ¸")
plt.title("ğŸŒ¸ Logistic Regression on Iris (Trained on 4 Features, PCA Visualized) ğŸŒ¸")
plt.grid(alpha=0.3)
plt.show()

print("âœ¨ Done! Be like a flower ğŸŒ¼â€”calm, bright, and blooming at your own pace ğŸ’–")
